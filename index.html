
<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Junsong Chen</title>

  <meta name="author" content="Junsong Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<!--  <link rel="icon" type="image/png" href="images/seal_icon.png">-->
</head>

<script async defer src="https://buttons.github.io/buttons.js"></script>
<tbody>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name style="font-size: 35px;">Junsong Chen</name>
              </p>
                <p>I am currently a research intern at <strong>AI Theory Lab of Huawei Noah's Ark Lab</strong> (Shen Zhen).
                    I am also a Research Assistant at <strong>The University of Hong Kong</strong> and a PhD Candidate at <strong>DLUT</strong>.
                    During my PhD time, I have been fortunate to work under the supervision of
                    <a href="https://scholar.google.com/citations?user=D3nE0agAAAAJ&hl=zh-CN">Prof. Huchuan Lu</a> and <a href="http://luoping.me/">Prof. Ping Luo</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:cjs1020440147@icloud.com">Email</a> &nbsp/&nbsp
                <a href="data/Resume_jsChen.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=p4zxPP8AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/lawrence-cj">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/jschen.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/jschen.png" class="hoverZoomLink"></a>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <h3 id=" Research Interests "> Research Interests </h3>
                <hr style="height:1px;border-width:0;color:gray;background-color:gray">
              <p>
                  <strong>In 2023</strong>, I participate in the following research topics:
                <ul>
                    <li> AIGC, e.g. Diffusion Models in 2D/3D </li>
                    <li> Large Language Models (LLMs), e.g.
                    <li> Autonomous Driving, e.g. 3D perception & prediction </li>
                </ul>
              </p>
              <p>
              <strong>Previously</strong>, I did research on 2D/3D detection, segmentation and object tracking. Representative works include <strong><a href="https://chongjiange.github.io/metabev.html">MetaBEV</a></strong> for 3D objection detection and segmentation and <strong><a href="https://arkittrack.github.io/">ARKitTrack</a></strong> for 2D object tracking.
              </p>
            </td>
          </tr>
        </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
          <br>
          <h3 id="news"> News </h3>
            <hr style="height:1px;border-width:0;color:gray;background-color:gray">
           <ul>
           <li>
                [01/2024] One paper was accepted by ICLR (Spotlight).
            </li>
            <li>
                [12/2023] One paper was accepted by AAAI.
            </li>
            <li>
                [07/2023] One paper was accepted by ICCV.
            </li>
            <li>
                [03/2023] One paper was accepted by CVPR.
            </li>
            </ul>
          </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
          <br>
          <h3 id="Selected Publications"> Selected Publications </h3>
            <hr style="height:1px;border-width:0;color:gray;background-color:gray">
          <p>
            For the latest full list please check <a href="https://scholar.google.com/citations?hl=en&user=p4zxPP8AAAAJ">here</a>.
          </p>
            <table><tbody></tbody></table>
          <h4 id="aigc"> AIGC (Text-to-Image Diffusion) </h4>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
                <img src="images/pixart-alpha.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://pixart-alpha.github.io/">
                <papertitle>PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</papertitle>
              </a>
              <br>
               <strong>Junsong Chen*</strong>,
               <a href="https://lovesykun.cn/about.html">Jincheng Yu*</a>,
               <a href="https://chongjiange.github.io/">Chongjian Ge*</a>,
               <a href="https://scholar.google.com/citations?user=hqDyTg8AAAAJ">Lewei Yao*</a>,
               <a href="https://xieenze.github.io/">Enze Xie</a>,
               <a href="https://yuewuhkust.github.io/">Yue Wu</a>,
               <a href="https://zhongdao.github.io/">Zhongdao Wang</a>,
               <a href="https://www.cse.ust.hk/~jamesk/">James Kwok</a>,
               <a href="http://luoping.me/">Ping Luo</a>,
               <a href="https://scholar.google.com/citations?hl=en&user=D3nE0agAAAAJ">Huchuan Lu</a>,
               <a href="https://scholar.google.com/citations?user=XboZC1AAAAAJ">Zhenguo Li</a>,
              <br>
              <em>ICLR</em>, 2024 (Spotlight)

              <br>
              <a href="https://pixart-alpha.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2310.00426">arXiv</a> /
              <a href="https://huggingface.co/PixArt-alpha">Demo</a> /
              <a href="https://github.com/PixArt-alpha/PixArt-alpha">code</a>  <a class="github-button" href="https://github.com/PixArt-alpha/PixArt-alpha" data-icon="octicon-star" data-show-count="true" aria-label="" target="_blank">star</a>

              <br>
              <p></p>
              <p> PixArt-α is a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), and the training speed markedly surpasses existing large-scale T2I models, e.g., PixArt-α only takes 12% of Stable Diffusion v1.5's training time (753 vs. 6,250 A100 GPU days). </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
          <h4 id="perception"> Perception (object detection/segmentation/tracking)  </h4>
            </td>
          </tr>
      </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
                <img src="images/metabev.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://chongjiange.github.io/metabev.html">
                <papertitle>MetaBEV: Solving Sensor Failures for BEV Detection and Map Segmentation</papertitle>
              </a>
              <br>
               <a href="https://chongjiange.github.io/">Chongjian Ge*</a>,
               <strong>Junsong Chen*</strong>,
               <a href="https://xieenze.github.io/">Enze Xie</a>,
               <a href="https://zhongdao.github.io/">Zhongdao Wang</a>,
               <a href="https://scholar.google.com/citations?user=2p7x6OUAAAAJ&hl=zh-CN">Lanqing Hong</a>,
               <a href="https://scholar.google.com/citations?user=D3nE0agAAAAJ&hl=en">Huchuan Lu</a>,
               <a href="https://scholar.google.com/citations?user=XboZC1AAAAAJ&hl=en">Zhenguo Li</a>,
               <a href="http://luoping.me/">Ping Luo</a>
              <br>
              <em>ICCV</em>, 2023

              <br>
              <a href="https://chongjiange.github.io/metabev.html">project page</a> /
              <a href="https://arxiv.org/abs/2304.09801">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=TiEQpYq77Xo&list=PLB9_L58NpyEWcJhnX-a09CRXp-2kNEojY&index=2">Youtube</a> /
              <a href="https://github.com/ChongjianGE/MetaBEV">code</a>  <a class="github-button" href="https://github.com/ChongjianGE/MetaBEV" data-icon="octicon-star" data-show-count="true" aria-label="" target="_blank">star</a>

              <br>
              <p></p>
              <p> MetaBEV is a 3D Bird-Eye's View (BEV) perception model that is robust to sensor missing/failure, supporting both single modality mode (camera/lidar) and multi-modal fusion mode with strong performance. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
                <img src="images/arkittrack.png" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arkittrack.github.io/">
                <papertitle>ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data</papertitle>
              </a>
              <br>
               <a href="https://scholar.google.com/citations?hl=en&user=rk1ozXMAAAAJ">Haojie Zhao*</a>,
               <strong>Junsong Chen*</strong>,
               <a href="http://faculty.dlut.edu.cn/wanglj/zh_CN/index.htm">Lijun Wang</a>,
               <a href="https://scholar.google.com/citations?user=D3nE0agAAAAJ&hl=en">Huchuan Lu</a>,
              <br>
              <em>CVPR</em>, 2023

              <br>
              <a href="https://arkittrack.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2303.13885">arXiv</a> /
              <a href="https://www.youtube.com/@user-hs6yq6du2u">Youtube</a> /
              <a href="https://github.com/lawrence-cj/ARKitTrack">code</a>  <a class="github-button" href="https://github.com/lawrence-cj/ARKitTrack" data-icon="octicon-star" data-show-count="true" aria-label="" target="_blank">star</a>

              <br>
              <p></p>
              <p> ARKitTrack, a new RGB-D track-ing dataset for both static and dynamic scenes captured by consumer-grade LiDAR scanners equipped on Apple’s iPhone and iPad.
                  ARKitTrack contains 300 RGB-D sequences, 455 targets, and 229.7K video frames in total. Along with the bounding box annotations and frame-level attributes.</p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</tbody>
</table>
</html>
